{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f0de1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06b907d",
   "metadata": {},
   "source": [
    "순환 신경망(Recurrent Neural Network, RNN)\n",
    "\n",
    "순환 신경망은 시퀀스(sequence) 데이터를 사용한다. 시퀀스 데이터는 다른 말로 시계열(time serise) 데이터라고도 말하며, 시점에 따라서 데이터가 달라지는 것을 의미한다. 즉, 특정 시점에서 데이터를 한 번에 수집하는 것이 아닌 시간의 흐름에 따라 데이터를 점차 수집하는 것을 의미한다. 전체 데이터셋을 구성하는 각 데이터 포인트의 수집 시점이 다르다.\n",
    "\n",
    "지금까지 다룬 신경망은 입력 데이터를 사용한 후 그 결과가 다시 입력층으로 돌아가지 않았다. 즉, 한 번 출력된 결과를 다시 사용되지 않았다. 이를 피드포워드(feedforward) 신경망이라 한다. 반면에 순환 신경망은 출력된 결과가 다음 시점에서 사용된다. 즉, 출력 결과를 다음 시점까지 기억했다가 사용하는 방식이다.\n",
    "\n",
    "<img src=\"./images/rnn_1.png/\" width=\"700\"/>\n",
    "\n",
    "LSTM(Long Short Term Memory)\n",
    "\n",
    "순환 신경망은 은닉층을 거친 결과값을 재사용하는 특징이 있다. 그러나 그로 인해서 기울기 소멸 문제(vanishing gradient problem)나 기울기 폭주 문제(exploding gradient problem)가 발생할 수 있다. 기울기 소멸 문제는 학습이 진행되는 과정에서 기울기 점점 줄어들어 사라지는 현상을 의미하고 기울기 폭주는 학습 과정에서 기울기가 점점 커져서 폭주하는 현상을 말한다.\n",
    "\n",
    "LSTM은 기울기 소멸이나 폭주 문제를 해결하기 위해 만든 방법이다. LSTM에서는 결과값이 다음 시점으로 넘어갈 때 결과값을 넘길지 말지 결정하는 단계가 추가된다.\n",
    "\n",
    "<img src=\"./images/rnn_2.png/\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69b51ef",
   "metadata": {},
   "source": [
    "순환 신경망 실습  \n",
    "순환 신경망을 이용해 영화 리뷰 감성 분석을 해본다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "853d31cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일관된 결과값이 나오도록 random seed를 설정한다.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c1eb9a",
   "metadata": {},
   "source": [
    "IMDB Movie Review Sentiment Analysis  \n",
    "감성 분류를 연습하기 위해 자주 사용하는 영어 데이터로 영화 사이트 IMBD 리뷰 데이터가 있다.  \n",
    "이 데이터는 리뷰에 대한 텍스트와 리뷰가 긍정인 경우 1을 부정인 경우 0으로 표시한 레이블로 구성된 데이터로 스탠포드 대학교에서 2011년에 낸 논문에서 이 데이터를 소개하였으며, 당시 논문에서는 이 데이터를 훈련 데이터와 테스트 데이터를 50:50 비율로 분할하여 88.89%의 정확도를 얻었다고 소개하고 있다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fec93b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "from tensorflow.keras import datasets # 텐서플로우가 제공하는 데이터셋을 사용하기 위해 import 한다.\n",
    "# mnist 손글씨 데이터는 트레이닝 데이터와 테스트 데이터가 나눠서 저장되어 있다.\n",
    "# load_data() 메소드는 (트레이닝 피쳐 데이터, 트레이닝 타겟 데이터)와 (테스트 피쳐 데이터, 테스트 타겟 데이터)를\n",
    "# 튜플 형태로 묶어서 리턴한다.\n",
    "(X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(num_words=2000) # 영화 리뷰 데이터셋을 피쳐, 타겟 데이터로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6cd6c0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
       "         list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "         list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
       "         ...,\n",
       "         list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 86527, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 15271, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 26094, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 17793, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 17793, 5, 27, 710, 117, 74936, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 17793, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 17793, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 22459]),\n",
       "         list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 21469, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 40691, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 29455, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 21213, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
       "         list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 16923, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 16553, 21, 27, 9685, 6139, 5, 29043, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 85010, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 70907, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 16876, 8, 1597, 8778, 19280, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
       "        dtype=object),\n",
       "  array([1, 0, 0, ..., 0, 1, 0], dtype=int64)),\n",
       " (array([list([1, 591, 202, 14, 31, 6, 717, 10, 10, 18142, 10698, 5, 4, 360, 7, 4, 177, 5760, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 7944, 100, 28, 1668, 14, 31, 23, 27, 7479, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 15387, 38, 32, 25, 7944, 451, 202, 14, 6, 717]),\n",
       "         list([1, 14, 22, 3443, 6, 176, 7, 5063, 88, 12, 2679, 23, 1310, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 7216, 10626, 4, 8463, 2801, 109, 1603, 21, 4, 22, 3861, 8, 6, 1193, 1330, 10, 10, 4, 105, 987, 35, 841, 16873, 19, 861, 1074, 5, 1987, 17975, 45, 55, 221, 15, 670, 5304, 526, 14, 1069, 4, 405, 5, 2438, 7, 27, 85, 108, 131, 4, 5045, 5304, 3884, 405, 9, 3523, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 8463, 2801, 45, 407, 31, 7, 41, 3778, 105, 21, 59, 299, 12, 38, 950, 5, 4521, 15, 45, 629, 488, 2733, 127, 6, 52, 292, 17, 4, 6936, 185, 132, 1988, 5304, 1799, 488, 2693, 47, 6, 392, 173, 4, 21686, 4378, 270, 2352, 4, 1500, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2078, 7, 5293, 861, 12746, 5, 4182, 30, 3127, 23651, 56, 4, 841, 5, 990, 692, 8, 4, 1669, 398, 229, 10, 10, 13, 2822, 670, 5304, 14, 9, 31, 7, 27, 111, 108, 15, 2033, 19, 7836, 1429, 875, 551, 14, 22, 9, 1193, 21, 45, 4829, 5, 45, 252, 8, 12508, 6, 565, 921, 3639, 39, 4, 529, 48, 25, 181, 8, 67, 35, 1732, 22, 49, 238, 60, 135, 1162, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 5687, 25, 203, 28, 8, 818, 12, 125, 4, 3077]),\n",
       "         list([1, 111, 748, 4368, 1133, 33782, 24563, 4, 87, 1551, 1262, 7, 31, 318, 9459, 7, 4, 498, 5076, 748, 63, 29, 5161, 220, 686, 10941, 5, 17, 12, 575, 220, 2507, 17, 6, 185, 132, 24563, 16, 53, 928, 11, 51278, 74, 4, 438, 21, 27, 10044, 589, 8, 22, 107, 20123, 19550, 997, 1638, 8, 35, 2076, 9019, 11, 22, 231, 54, 29, 1706, 29, 100, 18995, 2425, 34, 12998, 8738, 48078, 5, 19353, 98, 31, 2122, 33, 6, 58, 14, 3808, 1638, 8, 4, 365, 7, 2789, 3761, 356, 346, 4, 27608, 1060, 63, 29, 93, 11, 5421, 11, 15236, 33, 6, 58, 54, 1270, 431, 748, 7, 32, 2580, 16, 11, 94, 19469, 10, 10, 4, 993, 45222, 7, 4, 1766, 2634, 2164, 24563, 8, 847, 8, 1450, 121, 31, 7, 27, 86, 2663, 10760, 16, 6, 465, 993, 2006, 30995, 573, 17, 61862, 42, 4, 17345, 37, 473, 6, 711, 6, 8869, 7, 328, 212, 70, 30, 258, 11, 220, 32, 7, 108, 21, 133, 12, 9, 55, 465, 849, 3711, 53, 33, 2071, 1969, 37, 70, 1144, 4, 5940, 1409, 74, 476, 37, 62, 91, 1329, 169, 4, 1330, 10104, 146, 655, 2212, 5, 258, 12, 184, 10104, 546, 5, 849, 10333, 7, 4, 22, 1436, 18, 631, 1386, 797, 7, 4, 8712, 71, 348, 425, 4320, 1061, 19, 10288, 5, 12141, 11, 661, 8, 339, 17863, 4, 2455, 11434, 7, 4, 1962, 10, 10, 263, 787, 9, 270, 11, 6, 9466, 4, 61862, 48414, 121, 4, 5437, 26, 4434, 19, 68, 1372, 5, 28, 446, 6, 318, 7149, 8, 67, 51, 36, 70, 81, 8, 4392, 2294, 36, 1197, 8, 68411, 25399, 18, 6, 711, 4, 9909, 26, 10296, 1125, 11, 14, 636, 720, 12, 426, 28, 77, 776, 8, 97, 38, 111, 7489, 6175, 168, 1239, 5189, 137, 25399, 18, 27, 173, 9, 2399, 17, 6, 12397, 428, 14657, 232, 11, 4, 8014, 37, 272, 40, 2708, 247, 30, 656, 6, 13182, 54, 25399, 3292, 98, 6, 2840, 40, 558, 37, 6093, 98, 4, 17345, 1197, 15, 14, 9, 57, 4893, 5, 4659, 6, 275, 711, 7937, 25399, 3292, 98, 6, 31036, 10, 10, 6639, 19, 14, 10241, 267, 162, 711, 37, 5900, 752, 98, 4, 17345, 2378, 90, 19, 6, 73284, 7, 36744, 1810, 77553, 4, 4770, 3183, 930, 8, 508, 90, 4, 1317, 8, 4, 48414, 17, 15454, 3965, 1853, 4, 1494, 8, 4468, 189, 4, 31036, 6287, 5774, 4, 4770, 5, 95, 271, 23, 6, 7742, 6063, 21627, 5437, 33, 1526, 6, 425, 3155, 33697, 4535, 1636, 7, 4, 4669, 11966, 469, 4, 4552, 54, 4, 150, 5664, 17345, 280, 53, 68411, 25399, 18, 339, 29, 1978, 27, 7885, 5, 17303, 68, 1830, 19, 6571, 14605, 4, 1515, 7, 263, 65, 2132, 34, 6, 5680, 7489, 43, 159, 29, 9, 4706, 9, 387, 73, 195, 584, 10, 10, 1069, 4, 58, 810, 54, 14, 6078, 117, 22, 16, 93, 5, 1069, 4, 192, 15, 12, 16, 93, 34, 6, 1766, 28228, 33, 4, 5673, 7, 15, 18760, 9252, 3286, 325, 12, 62, 30, 776, 8, 67, 14, 17, 6, 12214, 44, 148, 687, 24563, 203, 42, 203, 24, 28, 69, 32157, 6676, 11, 330, 54, 29, 93, 61862, 21, 845, 14148, 27, 1099, 7, 819, 4, 22, 1407, 17, 6, 14967, 787, 7, 2460, 19569, 61862, 100, 30, 4, 3737, 3617, 3169, 2321, 42, 1898, 11, 4, 3814, 42, 101, 704, 7, 101, 999, 15, 1625, 94, 2926, 180, 5, 9, 9101, 34, 15205, 45, 6, 1429, 22, 60, 6, 1220, 31, 11, 94, 6408, 96, 21, 94, 749, 9, 57, 975]),\n",
       "         ...,\n",
       "         list([1, 13, 1408, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 5093, 21, 45, 184, 78, 4, 1492, 910, 769, 2290, 2515, 395, 4257, 5, 1454, 11, 119, 16946, 89, 1036, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2280, 284, 1842, 60664, 37, 315, 4, 226, 20, 272, 2942, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 5518]),\n",
       "         list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 3684, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2960, 7, 743, 46, 1028, 9, 3531, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 28739, 5, 9, 1861, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 1276, 92, 124, 51, 45, 1188, 71, 536, 13, 520, 14, 20, 6, 2302, 7, 470]),\n",
       "         list([1, 6, 52, 7465, 430, 22, 9, 220, 2594, 8, 28, 24357, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 5586, 18020, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 5586, 9, 133, 1810, 11, 5417, 13226, 21, 4, 7298, 42657, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 25194, 27803, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 20067, 5698, 3406, 718, 21264, 9, 6, 6907, 17, 210, 5, 3281, 5677, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 14274, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 16261, 19, 714, 727, 5205, 382, 4, 91, 6533, 439, 19, 14, 20, 9, 1441, 5805, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643])],\n",
       "        dtype=object),\n",
       "  array([0, 1, 1, ..., 0, 0, 0], dtype=int64)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2a42bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n",
      "(25000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "# 오리지널 데이터 확인\n",
    "# 트레이닝 피쳐 데이터의 차원을 확인하면 (이미지 개수, 행, 열) 형태로 나온다.\n",
    "# 즉, 트레이닝 피쳐 데이터는 28행 * 28열의 이미지 60,000개로 구성되어 있는 것을 알 수 있다.\n",
    "print(X_train.shape)\n",
    "print(y_train.shape) # 트레이닝 타겟 데이터는 스칼라값 60,000개로 이루어진 벡터이다.\n",
    "print(X_test.shape) # 테스트 피쳐 데이터는 28행 * 28열의 이미지 60,000개로 구성되어 있는 것을 알 수 있다.\n",
    "print(y_test.shape) # 테스트 타겟 데이터는 스칼라값 10,000개로 이루어진 벡터이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72b1819d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000,) (5000,)\n",
      "(20000,) (5000,)\n"
     ]
    }
   ],
   "source": [
    "# 트레이닝 데이터를 트레이닝 데이터셋과 벨리데이션 데이터셋으로 분리 \n",
    "# 총 25,000개의 트레이닝 피쳐 데이터 중 20,000개의 데이터를 트레이닝 피쳐 데이터로 지정한다.\n",
    "X_tr = X_train[:20000]\n",
    "# 총 25,000개의 트레이닝 피처 데이터 중에서 피쳐 데이터 20,000개를 제외하고 남은 5,000개의 트레이닝 피쳐 데이터를 \n",
    "# 밸리데이션 피쳐 데이터로 지정한다.\n",
    "X_valid = X_train[20000:]\n",
    "print(X_tr.shape, X_valid.shape)\n",
    "# 총 25,000개의 트레이닝 타겟 데이터 중 20,000개의 데이터를 트레이닝 타겟 데이터로 지정한다.\n",
    "y_tr = y_train[:20000]\n",
    "# 총 25,000개의 트레이닝 타겟 데이터 중에서 피쳐 데이터 20,000개를 제외하고 남은 5,000개의 트레이닝 피쳐 데이터를\n",
    "# 밸리데이션 타겟 데이터로 지정한다.\n",
    "y_valid = y_train[20000:]\n",
    "print(y_tr.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb68f372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 1920, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "# 피쳐 데이터 형태 확인\n",
    "# 트레이닝 피쳐 데이터 값을 확인해보면 숫자로 이루어진 리스트라는 것을 볼 수 있다.\n",
    "# 이는 오리지널 데이터에 이미 단어를 숫자 매핑해 놓아 영어 단어가 해당하는 숫자로 변환된 것임을 알 수 있다.\n",
    "print(X_tr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b563718b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n",
      "189\n"
     ]
    }
   ],
   "source": [
    "# 개별 피쳐 데이터 크기 확인\n",
    "# 피쳐 데이터의 가장 처음 나오는 두 개의 피쳐값을 확인해 본 결과 문장의 길이가 다르다는 것을 의미하며 나중에 같은\n",
    "# 크기로 조정이 필요한다.\n",
    "print(len(X_tr[0]))\n",
    "print(len(X_tr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f33feb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1} {0, 1}\n"
     ]
    }
   ],
   "source": [
    "# 타겟 클래스 확인\n",
    "# 타겟 클래스 구분은 0과 1로 2개로 이루어진 \n",
    "print(set(y_train), set(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3cce6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 100) (5000, 100) (25000, 100)\n"
     ]
    }
   ],
   "source": [
    "# 피쳐 데이터 변형\n",
    "# RNN은 항상 같은 길이의 시퀀스를 받아야하기 때문에 제각기 다른 피쳐 데이터 값의 길이를 일치한다. \n",
    "from tensorflow.keras.preprocessing import sequence # 피쳐 데이터 값의 길이를 일치시키기 위해 import 한다. \n",
    "# pad_sequences() 메소드로 피쳐 데이터 값의 길이를 일치시킨다. maxlen 속성으로 길이를 지정할 수 있다. \n",
    "# 길이를 지정하지 않으면 가장 큰 길이가 기본값으로 사용되고 길이를 맞추고 남는 부분에는 0이 패딩으로 채워진다. \n",
    "# 길이를 지정하면 지정한 길이 만큼만 저장되고 나머지는 삭제된다. \n",
    "X_tr = sequence.pad_sequences(X_tr, maxlen=100)\n",
    "X_valid = sequence.pad_sequences(X_valid, maxlen=100)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=100)\n",
    "print(X_tr.shape, X_valid.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e8239cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 신경망 생성에 필요한 라이브러리 import\n",
    "from tensorflow.keras.models import Sequential # 신경망 모형 생성을 하기 위해 import 한다.\n",
    "# 각종 층을 쌓기 위해 필요한 모듈을 import 한다.\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abc96dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 100)         200000    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 50)          15050     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, None, 50)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               60400     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 275,551\n",
      "Trainable params: 275,551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LSTM 모형을 만든다.\n",
    "model = Sequential()\n",
    "# 임베딩이란 사람이 쓰는 자연어를 기계가 이해할 수 있는 숫자의 나열인 벡터로 바꾼 결과 혹은 과정 전체를 의미한다.\n",
    "# 임베딩이 가장 간단한 형태는 단어의 빈도를 그대로 벡터로 사용하는 것이다.\n",
    "# 총 2,000개의 단어를 사용하므로 input_dim 옵션은 2,000을 지정하고 데이터 길이는 pad_sequences() 함수를 사용해 \n",
    "# 100개로 맞췄으므로 output_dim 옵션은 100을 지정한다.\n",
    "model.add(Embedding(input_dim=2000, output_dim=100)) # 단어 임베딩 층을 추가한다.\n",
    "# Param은 input_dim * output_dim = 2000 * 100 => 200000\n",
    "# CNN은 2차원 데이터를 이용하서 Conv2D를 사용했지만 LSTM은 1차원 데이터가 입력되므로 Conv1D를 사용한다.\n",
    "model.add(Conv1D(filters=50, kernel_size=3, padding='valid', activation='relu')) # 합성곱 층을 추가한다.\n",
    "# Param은 커널 크기(3) * 입력 채널 수(100) * 필터 수(50) + 출력 채널 바이어스(50) = 3 * 100 * 50 + 50 = 15050\n",
    "model.add(MaxPooling1D(pool_size=3)) # 맥스 풀링 층을 추가한다.\n",
    "model.add(LSTM(units=100, activation='tanh')) # LSTM 층을 추가한다. LSTM 층은 4개이다.\n",
    "# Param은 (출력 개수(100) + 입력 개수(50) + 바이어스(1)) * 출력 개수(100) * 4 = (100 + 50 + 1) * 100 * 4\n",
    "# = 151 * 100 * 4 = 60400\n",
    "model.add(Dropout(0.25)) # 드롭아웃 층을 추가한다.\n",
    "model.add(Dense(1, activation='sigmoid')) # 최종 출력층을 추가해주면 순환 신경망이 완성된다.\n",
    "# # Param은 입력 개수(100) * 출력 개수(1) + 출력 개수와 같은 바이어스(1) = 100 * 1 + 1 = 101\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64e1d086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일\n",
    "# 분류 신경망은 loss 속성에 손실(비용, 오차)함수를 지정할 때 이진 분류 문제 binary_crossentropy를 지정하고, \n",
    "# 3개 이상의 클래스로 분류하는 다중 분류 문제에서는 categorical_crossentropy를 지정한다.\n",
    "# optimizer 속성에 최적화 함수(일반적으로 adam)를 지정한다.\n",
    "# metrics 속성에 평가 기준을 지정한다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "623ec07d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "200/200 [==============================] - 14s 59ms/step - loss: 0.4879 - accuracy: 0.7466 - val_loss: 0.3845 - val_accuracy: 0.8294\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 12s 61ms/step - loss: 0.3418 - accuracy: 0.8518 - val_loss: 0.3718 - val_accuracy: 0.8308\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 13s 64ms/step - loss: 0.3009 - accuracy: 0.8747 - val_loss: 0.3724 - val_accuracy: 0.8348\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 12s 61ms/step - loss: 0.2584 - accuracy: 0.8964 - val_loss: 0.3791 - val_accuracy: 0.8344\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 12s 59ms/step - loss: 0.2062 - accuracy: 0.9227 - val_loss: 0.4110 - val_accuracy: 0.8266\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 12s 60ms/step - loss: 0.1366 - accuracy: 0.9513 - val_loss: 0.4686 - val_accuracy: 0.8234\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 12s 61ms/step - loss: 0.0861 - accuracy: 0.9714 - val_loss: 0.6753 - val_accuracy: 0.8152\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 12s 62ms/step - loss: 0.0532 - accuracy: 0.9832 - val_loss: 0.7057 - val_accuracy: 0.8158\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 12s 60ms/step - loss: 0.0279 - accuracy: 0.9919 - val_loss: 0.8629 - val_accuracy: 0.8072\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 12s 58ms/step - loss: 0.0201 - accuracy: 0.9944 - val_loss: 0.9193 - val_accuracy: 0.8104\n"
     ]
    }
   ],
   "source": [
    "# 신경망 학습\n",
    "hist = model.fit(X_tr, y_tr, epochs=10, batch_size=100, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d6fa85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 5s 8ms/step - loss: 0.0071 - accuracy: 0.9987\n",
      "[0.007111974526196718, 0.9986500144004822]\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 0.9193 - accuracy: 0.8104\n",
      "[0.9193267822265625, 0.8104000091552734]\n",
      "782/782 [==============================] - 5s 7ms/step - loss: 0.8959 - accuracy: 0.8174\n",
      "[0.8958622813224792, 0.817359983921051]\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "print(model.evaluate(X_tr, y_tr))\n",
    "print(model.evaluate(X_valid, y_valid))\n",
    "print(model.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc93331b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13852\\1724543467.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# 정확도 시각화\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 정확도를 출력할 서브 플롯\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'b'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'o'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train_acc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r--'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'^'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'valid_acc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'epoch' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc_train = hist.history['accuracy'] # 트레이닝 데이터의 정확도를 저장한다.\n",
    "acc_valid = hist.history['val_accuracy'] # 밸리데이션 데이터의 정확도를 저장한다.\n",
    "loss_train = hist.history['loss'] # 트레이닝 데이터의 손실 정도를 저장한다.\n",
    "loss_valid = hist.history['val_loss'] # 밸리데이션 데이터의 손실 정도를 저장한다.\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "# 정확도 시각화 \n",
    "plt.subplot(1, 2, 1) # 정확도를 출력할 서브 플롯\n",
    "plt.plot(epoch, acc_train, 'b', marker='o', label='train_acc')\n",
    "plt.plot(epoch, acc_valid, 'r--', marker='^', label='valid_acc')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "# 손실 시각화.\n",
    "plt.subplot(1, 2, 2) # 손실을 출력할 서브 플롯\n",
    "plt.plot(epoch, loss_train, 'b', marker='o', label='train_loss')\n",
    "plt.plot(epoch, loss_valid, 'r--', marker='^', label='valid_loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11592548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a38c3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
